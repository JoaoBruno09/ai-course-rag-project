{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoBruno09/ai-course-rag-project/blob/master/RAG_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Project -> Financial Analysis\n",
        "\n",
        "#### THE RAG FLOW:\n",
        "  1. Retrieve: Get relevant documents (this case pdf documents)\n",
        "  2. Format: Combine documents into context string\n",
        "  3. Prompt: Create structured instruction for LLM\n",
        "  4. Chain: prompt -> llm -> output parser\n",
        "  5. Generation: LLM produces natural language answer\n",
        "  6. Return: User gets readable answer\n",
        "\n",
        "  ---\n",
        "\n",
        "#### THIS RAG FEATURES:\n",
        "- Injestion function\n",
        "- Inference function\n",
        "- Similarity Search with separators\n",
        "- Chroma persistence\n",
        "- LCEL (LangChain Expression Language) chains\n",
        "- Prompt templates\n",
        "- LLM integration\n",
        "- Gradio ChatInterface\n",
        "- Auto-detection year or report quarterly period using LLM classification model\n",
        "- One-at-a-time injestion\n",
        "- Text preprocessing\n",
        "- Metadata filtering\n",
        "\n",
        "Vector Database used: https://www.trychroma.com"
      ],
      "metadata": {
        "id": "DDEMjOnRY_i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "YceOZZzJZPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain==0.3.27\" -qqq\n",
        "%pip install \"langchain-community==0.3.31\" -qqq\n",
        "%pip install \"langchain-openai==0.3.35\" -qqq\n",
        "%pip install \"langchain-chroma==0.2.6\" -qqq\n",
        "%pip install pypdf -qqq\n",
        "%pip install gradio -qqq"
      ],
      "metadata": {
        "id": "0h03ZTtdZUop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f608163-160a-4157-d7a9-9d2941283773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "yV2DhobPawAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"CHROMA_API_KEY\"] = userdata.get(\"CHROMA_API_KEY\")\n",
        "os.environ[\"CHROMA_TENANT\"] = userdata.get(\"CHROMA_TENANT\")\n",
        "os.environ[\"DB_NAME\"] = userdata.get(\"DB_NAME\")\n",
        "\n",
        "# Version Management\n",
        "REPORTS_VERSION = \"v1\"\n",
        "\n",
        "# Vector Databases collection name\n",
        "REPORTS_COLLECTION_NAME = f\"googl_reports_{REPORTS_VERSION}\"\n"
      ],
      "metadata": {
        "id": "ReJeaLCWa1RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nmxmjjK4c9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Embeddings Model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0 # Deterministic for consistent classification\n",
        ")\n",
        "\n",
        "# Model for report year classififcation\n",
        "classification_llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0  # Deterministic for consistent classification\n",
        ")\n",
        "\n",
        "# Vector Databases\n",
        "reports_vectorstore = Chroma(\n",
        "  embedding_function=embeddings,\n",
        "  collection_name=REPORTS_COLLECTION_NAME,  # Version-based naming\n",
        "  chroma_cloud_api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
        "  tenant=os.getenv(\"CHROMA_TENANT\"),\n",
        "  database=os.getenv(\"DB_NAME\")\n",
        ")"
      ],
      "metadata": {
        "id": "SZmapQgqc9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Injestion Process"
      ],
      "metadata": {
        "id": "hXssSQf8gCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean text function\n",
        "\n",
        "import re\n",
        "\n",
        "def improvePage(text: str) -> str:\n",
        "  \"\"\"\n",
        "    Method that applies text preprocessing\n",
        "    - Remove hyphenated line breaks\n",
        "    - Remove multiple whitespaces\n",
        "    - Normalize punctuation spacing\n",
        "    - Remove standalone page numbers\n",
        "    - Remove whitespace from both the beginning and the end of the string\n",
        "\n",
        "    Args:\n",
        "      documentPath (str): path to Document\n",
        "\n",
        "    Returns:\n",
        "      str: Processed text\n",
        "  \"\"\"\n",
        "\n",
        "  # Remove hyphenated line breaks\n",
        "  text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)\n",
        "\n",
        "  # Remove multiple whitespaces -> \\s+ matches one or more whitespace characters (spaces, tabs, newlines) and replaces them with a single space \" \"\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "  # Normalize punctuation spacing\n",
        "  text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
        "  text = re.sub(r'([.,!?;:])([^\\s])', r'\\1 \\2', text)\n",
        "\n",
        "  # Remove standalone page numbers -> remove a trailing number only if it's the final token\n",
        "  text = re.sub(r'(?<=\\.)\\s*\\d+\\s*$', '', text)\n",
        "\n",
        "  # Removes whitespace from both the beginning and the end of the string\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "Pz1A-MEjQDhB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Document Classification Chain Creation\n",
        "\n",
        "def detect_document_properties(report):\n",
        "  \"\"\"\n",
        "  Method retuns the document properties classified by LLM\n",
        "  - Prompt to detect report year and quarterly period\n",
        "  - Create detection chain and invoke it\n",
        "  - Returns JSON with detected values\n",
        "\n",
        "  Args:\n",
        "    report: Document report loaded by PDF\n",
        "\n",
        "  Returns:\n",
        "    detection_result: JSON with detected values\n",
        "  \"\"\"\n",
        "\n",
        "  # Detection Prompt\n",
        "  detection_template = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are a world-class **Financial Document Analyst** and **Stock Market Expert** specializing in reading and interpreting corporate annual and quarterly reports.\n",
        "\n",
        "    Your task is to extract two specific pieces of information from the following financial report text:\n",
        "\n",
        "    1. **Fiscal Year** – the year the report covers (e.g., 2023).\n",
        "    2. **Fiscal Quarter** – the quarter the report covers, if applicable (Q1, Q2, Q3, Q4).\n",
        "      - If the document is an **annual report**, set Quarter to `\"ANNUAL\"`.\n",
        "      - If quarter cannot be determined, set Quarter to `\"UNKNOWN\"`.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Step-by-Step Instructions\n",
        "    1. Read the provided document text carefully.\n",
        "    2. Identify the fiscal or calendar year the report refers to (e.g., “for the year ended December 31, 2023” → year = 2023, “For the quarterly period ended June 30, 2025” → year = 2025).\n",
        "    3. Identify the quarter, if mentioned (e.g., “Quarter ended March 31, 2023” → quarter = Q1, “For the quarterly period ended June 30, 2025” → quarter = Q2).\n",
        "    4. If multiple years or quarters appear, choose the **most recent** one that the report explicitly covers.\n",
        "    5. Output the results **strictly in JSON format** as follows: {{\"year\": \"<4-digit-year>\", \"quarter\": \"<Q1|Q2|Q3|Q4|ANNUAL|UNKNOWN>\"}}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Document Content\n",
        "    {content}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Final Answer:\n",
        "    Return **only** the JSON block — no extra text or commentary.\n",
        "    \"\"\"\n",
        "  )\n",
        "\n",
        "  # Detection Chain\n",
        "  detection_chain = detection_template | classification_llm | StrOutputParser()\n",
        "\n",
        "  # Detection Document Sample\n",
        "  sample_content = \"\"\n",
        "  for doc in report[:1]:  # First page\n",
        "      sample_content += doc.page_content + \"\\n\\n\"\n",
        "\n",
        "  # Limit to 500 characters to save costs, but also to identify report year, report quarter and ticker\n",
        "  sample_content = sample_content[:500]\n",
        "\n",
        "  # Invoke chain\n",
        "  detection_result = detection_chain.invoke({\n",
        "      \"content\": sample_content\n",
        "  }).strip().lower()\n",
        "\n",
        "  return detection_result"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yaRx-xNm8sD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Injestion Function\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import re\n",
        "import json\n",
        "\n",
        "def ingest_report(documentPath: str):\n",
        "  \"\"\"\n",
        "  Injestion Function - Run Once (or when data changes)\n",
        "\n",
        "  Injestion Features:\n",
        "  - Loads pdf reports using PyPDF loader\n",
        "  - Auto-detect report year and quarterly period if applicable using a classification LLM (gpt-3.5-turbo)\n",
        "  - Applies text processing\n",
        "  - Creates chunks using RecursiveCharacterTextSplitter according to separators\n",
        "  - Adds report year and quarterly period if applicable to each chunk metadata\n",
        "  - Creates or appends to chroma collection\n",
        "\n",
        "  Args:\n",
        "      documentPath (str): path to Document\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"-\" * 80)\n",
        "  print(f\"STARTING ADBE ANNUAL REPORT '{documentPath}' INGESTION PIPELINE - VERSION {REPORTS_VERSION}\")\n",
        "  print(\"-\" * 80)\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 1: LOAD REPORTS\n",
        "  #--------------------------------------------------------------------------------\n",
        "  print(\"\\n[1/6] Loading report...\")\n",
        "\n",
        "  loader = PyPDFLoader(documentPath)\n",
        "  report = loader.load()\n",
        "\n",
        "  print(f\"✓ Loaded {len(report)} pages from file\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 2: AUTO-DETECT YEAR AND QUARTER (IF APPLICABLE) REPORT\n",
        "  #--------------------------------------------------------------------------------\n",
        "  print(f\"\\n[2/6] Auto-detecting report year and quarterly period...\")\n",
        "\n",
        "  detection_result = detect_document_properties(report).strip()\n",
        "  detected_year = json.loads(detection_result)[\"year\"]\n",
        "  detected_quarter = json.loads(detection_result)[\"quarter\"]\n",
        "\n",
        "  print(f\"✓ Financial Report Year auto-detected: '{detected_year}'\")\n",
        "  print(f\"✓ Financial Report Quarterly Period auto-detected: '{detected_quarter}'\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 3: PREPROCESSING TEXT\n",
        "  #--------------------------------------------------------------------------------\n",
        "\n",
        "  print(f\"\\n[3/6] Applying text preprocessing...\")\n",
        "\n",
        "  for doc in report:\n",
        "    doc.page_content = improvePage(doc.page_content)\n",
        "\n",
        "  print(f\"✓ Cleaned {len(report)} pages\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 4: CHUNK DOCUMENT\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # WHY CHUNK?\n",
        "  # - LLMs have token limits (context window)\n",
        "  # - Smaller chunks = more precise retrieval\n",
        "  # - Balance: too small (lose context) vs too large (lose precision)\n",
        "\n",
        "  print(f\"\\n[4/6] Chunking document...\")\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=2000,\n",
        "      chunk_overlap=250, #18% of 2000 chunk size\n",
        "      separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"], # how the chunks are separeted\n",
        "  )\n",
        "\n",
        "  chunks = text_splitter.split_documents(report)\n",
        "\n",
        "  print(f\"✓ Split into {len(chunks)} chunks\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 5: ADD METADATA\n",
        "  #--------------------------------------------------------------------------------\n",
        "  print(f\"\\n[5/6] Enriching chunks with metadata...\")\n",
        "\n",
        "  for chunk in chunks:\n",
        "    # ADD new metadata (doesn't override existing)\n",
        "    chunk.metadata.update({\n",
        "        'report_year': detected_year,\n",
        "        'report_quarter': detected_quarter\n",
        "    })\n",
        "\n",
        "  print(f\"✓ Metadata enriched for all chunks:\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 5: CREATE EMBEDDINGS AND STORE IN CHROMA\n",
        "  #--------------------------------------------------------------------------------\n",
        "\n",
        "  print(f\"\\n[6/6] Creating embeddings and storing in Chroma...\")\n",
        "\n",
        "  reports_vectorstore.add_documents(documents=chunks)\n",
        "\n",
        "  print(f\"✓ Embeddings created and stored\")"
      ],
      "metadata": {
        "id": "c9JY2CdS9A3X",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Injestion\n",
        "\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2018.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2019.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2020.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2021.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2022.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2023.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2024.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2025_1Q.pdf\")\n",
        "ingest_report(\"/content/NASDAQ_GOOGL_2025_2Q.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHP6fpIzlj9B",
        "outputId": "dec83896-3d9b-4216-f130-ea31a072ea4d",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STARTING ADBE ANNUAL REPORT '/content/NASDAQ_GOOGL_2018.pdf' INGESTION PIPELINE - VERSION v1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/6] Loading report...\n",
            "✓ Loaded 107 pages from file\n",
            "\n",
            "[2/6] Auto-detecting report year and quarterly period...\n",
            "✓ Financial Report Year auto-detected: '2017'\n",
            "✓ Financial Report Quarterly Period auto-detected: 'annual'\n",
            "\n",
            "[3/6] Applying text preprocessing...\n",
            "✓ Cleaned 107 pages\n",
            "\n",
            "[4/6] Chunking document...\n",
            "✓ Split into 240 chunks\n",
            "\n",
            "[5/6] Enriching chunks with metadata...\n",
            "✓ Metadata enriched for all chunks:\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STARTING ADBE ANNUAL REPORT '/content/NASDAQ_GOOGL_2019.pdf' INGESTION PIPELINE - VERSION v1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/6] Loading report...\n",
            "✓ Loaded 99 pages from file\n",
            "\n",
            "[2/6] Auto-detecting report year and quarterly period...\n",
            "✓ Financial Report Year auto-detected: '2018'\n",
            "✓ Financial Report Quarterly Period auto-detected: 'annual'\n",
            "\n",
            "[3/6] Applying text preprocessing...\n",
            "✓ Cleaned 99 pages\n",
            "\n",
            "[4/6] Chunking document...\n",
            "✓ Split into 233 chunks\n",
            "\n",
            "[5/6] Enriching chunks with metadata...\n",
            "✓ Metadata enriched for all chunks:\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "STARTING ADBE ANNUAL REPORT '/content/NASDAQ_GOOGL_2025_2Q.pdf' INGESTION PIPELINE - VERSION v1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/6] Loading report...\n",
            "✓ Loaded 60 pages from file\n",
            "\n",
            "[2/6] Auto-detecting report year and quarterly period...\n",
            "✓ Financial Report Year auto-detected: '2025'\n",
            "✓ Financial Report Quarterly Period auto-detected: 'q2'\n",
            "\n",
            "[3/6] Applying text preprocessing...\n",
            "✓ Cleaned 60 pages\n",
            "\n",
            "[4/6] Chunking document...\n",
            "✓ Split into 117 chunks\n",
            "\n",
            "[5/6] Enriching chunks with metadata...\n",
            "✓ Metadata enriched for all chunks:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Process"
      ],
      "metadata": {
        "id": "61s-geB4kdqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Query Classification Chain Creation\n",
        "\n",
        "\"\"\"\n",
        "  Method retuns the query properties classified by LLM\n",
        "  - Prompt to detect report year and quarterly period\n",
        "  - Create detection chain and invoke it\n",
        "  - Returns JSON with detected values\n",
        "\n",
        "  Args:\n",
        "    query: User query\n",
        "\n",
        "  Returns:\n",
        "    year_quarter_report: JSON with detected values\n",
        "\"\"\"\n",
        "\n",
        "def detect_query_properties(query):\n",
        "\n",
        "  # Year or Quarter Detection Prompt\n",
        "  classification_template = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are a precise text classification model specialized in identifying the **reporting period** referenced in a financial question. You must extract the relevant year and quarterly period the user is referring to.\n",
        "\n",
        "    Your goal is to classify the user's question by identifying each **year** and **quarterly period** mentioned. Classify according to the following categories:\n",
        "\n",
        "    - **2025** → Refers to a specific **year** (e.g., “2025,” “next year,” “2025 performance”)\n",
        "    - **1Q** → Refers to the **first quarter** (Q1, January–March, or similar references)\n",
        "    - **2Q** → Refers to the **second quarter** (Q2, April–June, or similar references)\n",
        "    - **3Q** → Refers to the **third quarter** (Q3, July–September, or similar references)\n",
        "    - **4Q** → Refers to the **fourth quarter** (Q4, October–December, or similar references)\n",
        "    - **ANNUAL** → Refers to a **full-year** or **annual report** (e.g., “annual report,” “full-year,” “yearly performance,” or references to the company’s complete fiscal year)\n",
        "    - **UNKNOWN** → If the question does not clearly specify any year or quarter.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Step-by-Step Instructions\n",
        "    - If there is any **ambiguity** or the period is **not clear**, output **UNKNOWN**.\n",
        "    - Ensure that the classification is **direct** and **precise** without explanation.\n",
        "    - Output the results **strictly in JSON format** as follows: {{\"user-year\": \"<4-digit-year>\", \"user-quarter\": \"<Q1|Q2|Q3|Q4|ANNUAL|UNKNOWN>\"}}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### USER:\n",
        "    Question: {query}\n",
        "\n",
        "    ### Classification:\n",
        "    Return **only** the JSON block — no extra text or commentary.\n",
        "    \"\"\"\n",
        "  )\n",
        "\n",
        "  # Year or Quarter Detection Chain\n",
        "  classification_chain = classification_template | classification_llm | StrOutputParser()\n",
        "\n",
        "  # Invoke chain\n",
        "  year_quarter_report = classification_chain.invoke({\n",
        "      \"query\": query\n",
        "  }).strip().lower()\n",
        "\n",
        "  return year_quarter_report"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UWFpWq6K-ILb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RAG Chain Creation\n",
        "\n",
        "\"\"\"\n",
        "  Method retuns the RAG chain\n",
        "  - Prompt to analyse stock market reports according to user query\n",
        "  - Create a RAG chain and return it\n",
        "\n",
        "  Returns:\n",
        "    chain: created RAG chain\n",
        "\"\"\"\n",
        "\n",
        "def create_rag_chain():\n",
        "  prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are a world-class **Stock Analyst** and **Financial Report Specialist** trained in **fundamental and value investing** following the philosophies of **Warren Buffett**, **Peter Lynch**, and **Benjamin Graham**.\n",
        "\n",
        "    Your expertise lies in reading **corporate annual, quarterly, and earnings reports** to evaluate a company’s **intrinsic business quality and long-term value**.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Your Mission\n",
        "    Based **only** on the reports provided below, you must:\n",
        "\n",
        "    1. Provide a **professional summary and analysis** of the company.\n",
        "    2. Describe the company’s **financial performance year by year**, including revenue, earnings, margins, cash flow, debt, and other key metrics.\n",
        "    3. **Answer the user’s question** clearly and concisely, in the **same language** as the question.\n",
        "    4. Assign a **Company Evaluation Score (0–100)** according to **Buffett–Lynch–Graham value-investing principles**.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Evaluation Principles\n",
        "\n",
        "    Use these guiding principles when scoring:\n",
        "\n",
        "    **Warren Buffett – Business Quality & Moat**\n",
        "    - Understandable business model (“circle of competence”)\n",
        "    - Consistent earnings and return on equity\n",
        "    - Sustainable competitive advantage (economic moat)\n",
        "    - Honest, capable management\n",
        "\n",
        "    **Peter Lynch – Growth at a Reasonable Price (GARP)**\n",
        "    - Clear growth story grounded in fundamentals\n",
        "    - Reasonable valuation relative to earnings and assets\n",
        "    - Healthy financial structure (manageable debt, solid cash flow)\n",
        "\n",
        "    **Benjamin Graham – Margin of Safety & Intrinsic Value**\n",
        "    - Stock price meaningfully below intrinsic value\n",
        "    - Conservative accounting and prudent capital allocation\n",
        "    - Strong balance sheet and predictable profits\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Scoring Framework\n",
        "\n",
        "    | Score Range | Interpretation |\n",
        "    |--------------|----------------|\n",
        "    | **0–20** | Very weak fundamentals, no margin of safety, deteriorating business |\n",
        "    | **21–40** | Weak financials or overvaluation, limited safety margin |\n",
        "    | **41–60** | Average fundamentals, fairly valued, moderate stability |\n",
        "    | **61–80** | Strong fundamentals, good margin of safety, stable growth |\n",
        "    | **81–100** | Exceptional quality, durable moat, undervalued or superior long-term compounding potential |\n",
        "\n",
        "    Consider only what appears in the reports, market data, or company news provided — never speculate beyond that information.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Compliance Rules\n",
        "    - **Do NOT** give trading or investment recommendations (no “buy,” “sell,” or “hold”).\n",
        "    - **Do NOT** use external data or personal opinions.\n",
        "    - If the question is **not related to the reports**, respond exactly with:\n",
        "      > \"The question is not related to the provided reports.\"\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Input Variables\n",
        "\n",
        "    **Reports:**\n",
        "    {context}\n",
        "\n",
        "    **Question:**\n",
        "    {query}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### Output Instructions\n",
        "    Respond in the **same language** as the question.\n",
        "    Structure your answer in **four parts**:\n",
        "\n",
        "    1. **Introduction** – Brief overview of the company, business model, and recent financial context.\n",
        "    2. **Financial Analysis** – Summarize revenue, net income, operating margins, cash flow, debt levels, and other key metrics. Highlight growth trends, anomalies, or financial strengths/weaknesses. If possible refer the fiscal year that each metric is based on and compare the growth along of the years.\n",
        "    3. **Qualitative Analysis** – Evaluate the company’s competitive advantage, management quality, and alignment with value investing principles.\n",
        "    4. **Evaluation Score** – Assign a **Company Evaluation Score (0–100)** based strictly on Buffett–Lynch–Graham criteria.\n",
        "      > **Evaluation Score:** <numeric_value>\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### ✅ Final Answer:\n",
        "    \"\"\"\n",
        "  )\n",
        "\n",
        "  print(\"✓ Prompt template created\")\n",
        "\n",
        "  # The pipe (|) operator connects components (output from the last is the input of the next)\n",
        "  # Read left to right: prompt → llm → parser\n",
        "\n",
        "  print(\"\\n[5/6] Composing chain...\")\n",
        "\n",
        "  chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "  return chain"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6dkjisMQBark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inference function\n",
        "\n",
        "def inference(query: str):\n",
        "  \"\"\"\n",
        "  INFERENCE PIPELINE - Run Per User Query\n",
        "\n",
        "  Inference Features:\n",
        "  - Auto-detect report year and quarterly period if applicable using a classification LLM (gpt-3.5-turbo)\n",
        "  - Build metadata filter\n",
        "  - Similarity search with metadata filter\n",
        "  - Context formatting for LLM\n",
        "  - Prompt Engineering for RAG Chain\n",
        "  - LLM generation response\n",
        "\n",
        "  Args:\n",
        "      query (str): User's question\n",
        "\n",
        "  Returns:\n",
        "      str: Natural language answer\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"=\"*80)\n",
        "  print(f\"RUNNING INFERENCE - VERSION {REPORTS_VERSION}\")\n",
        "  print(\"=\"*80)\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 1:  AUTO-DETECT YEAR AND QUARTER (IF APPLICABLE) REPORT\n",
        "  #--------------------------------------------------------------------------------\n",
        "  print(f\"\\n[1/6] Detecting report year...\")\n",
        "\n",
        "  detection_result = detect_query_properties(query)\n",
        "  user_detected_year = json.loads(detection_result)[\"user-year\"]\n",
        "  user_detected_quarter = json.loads(detection_result)[\"user-quarter\"]\n",
        "\n",
        "  print(f\"✓ User question year auto-detected: '{user_detected_year}'\")\n",
        "  print(f\"✓ User question quarterly period auto-detected: '{user_detected_quarter}'\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 2: BUILD FILTER\n",
        "  #--------------------------------------------------------------------------------\n",
        "  print(f\"\\n[2/6] Building metadata filter...\")\n",
        "\n",
        "  filter_conditions = {}\n",
        "\n",
        "  # Add year filter if the detected year is not 'UNKNOWN'\n",
        "  if user_detected_year.lower() != 'UNKNOWN'.lower():\n",
        "      filter_conditions['report_year'] = {'$eq': user_detected_year}\n",
        "\n",
        "  # Add quarter filter if the detected quarter is not 'UNKNOWN'\n",
        "  if user_detected_quarter.lower() != 'UNKNOWN'.lower():\n",
        "      filter_conditions['report_quarter'] = {'$eq': user_detected_quarter}\n",
        "\n",
        "  # Wrap the filter conditions in an '$and' operator to combine them\n",
        "  if len(filter_conditions) > 1:\n",
        "    filter_conditions = {'$or': [{key: value} for key, value in filter_conditions.items()]}\n",
        "\n",
        "  print(f\"Final filter: {filter_conditions or 'None'}\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 3: SIMILARITY SEARCH WITH METADATA FILTER\n",
        "  #--------------------------------------------------------------------------------\n",
        "\n",
        "  print(f\"\\n[3/6] Performing similarity search...\")\n",
        "  print(f\"  Query: '{query}'\")\n",
        "\n",
        "  if filter_conditions:\n",
        "    results = reports_vectorstore.similarity_search(query, k=100, filter=filter_conditions)\n",
        "  else:\n",
        "    results = reports_vectorstore.similarity_search(query, k=100)\n",
        "\n",
        "  print(f\"✓ Found {len(results)} relevant chunks\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 4: FORMAT CONTEXT\n",
        "  #--------------------------------------------------------------------------------\n",
        "\n",
        "  print(f\"\\n[4/6] Formatting context for LLM...\")\n",
        "\n",
        "  context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "  print(f\"\\n✓ Context formatted for LLM...\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 5: PROMPT TEMPLATE AND COMPOSE CHAIN\n",
        "  #--------------------------------------------------------------------------------\n",
        "\n",
        "  print(\"\\n[5/6] Creating prompt template and composing chain...\")\n",
        "\n",
        "  chain = create_rag_chain()\n",
        "\n",
        "  print(\"\\n✓ Created prompt and Chain composed!\")\n",
        "  #print(\"\\n  Chain structure: \\n prompt_template  (formats variables) -> llm (generates response) -> output_parser (extracts string)\")\n",
        "\n",
        "  #--------------------------------------------------------------------------------\n",
        "  # STEP 6: GENERATE ANSWER BY INVOKING CHAIN\n",
        "  #--------------------------------------------------------------------------------\n",
        "\n",
        "  print(f\"\\n[6/7] Invoking RAG chain with context and query...\")\n",
        "\n",
        "  # Pass variables as dictionary to the chain\n",
        "  response = chain.invoke({\n",
        "      \"context\": context,  # Retrieved reports\n",
        "      \"query\": query       # User's question\n",
        "  }, tools=[{\"type\": \"text\"}])\n",
        "\n",
        "  print(f\"\\n✓ Answer generated ({len(response)} characters)\")\n",
        "\n",
        "  print(\"\\n\" + \"=\"*80)\n",
        "  print(\"INFERENCE COMPLETE\")\n",
        "  print(\"=\"*80)\n",
        "\n",
        "  return response  # Returns string response as natural language answer"
      ],
      "metadata": {
        "id": "JE72OpGlkfCV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Inference"
      ],
      "metadata": {
        "id": "E8bKt_ZRmRC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_inference(message, history):\n",
        "    \"\"\"\n",
        "    Gradio ChatInterface wrapper\n",
        "\n",
        "    Args:\n",
        "        message (str): Current user message\n",
        "\n",
        "    Returns:\n",
        "        str: Bot response\n",
        "    \"\"\"\n",
        "\n",
        "    return inference(message)"
      ],
      "metadata": {
        "id": "llC7izGAJTy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "#query = \"Analisa a GOOGL financeiramente\"\n",
        "#res = inference(query)\n",
        "#print(res)\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_inference,\n",
        "    type=\"messages\",\n",
        "    title=\"Stock Analysis RAG\",\n",
        "    description=\"Ask questions about Google stock.\",\n",
        "    examples=[\n",
        "        \"Summarize the Q2 2024 financial performance of Googl.\",\n",
        "        \"Compare the revenue growth of Googl over the past two years.\",\n",
        "        \"What are the main risks highlighted in Googl’s 2023 annual report?\",\n",
        "        \"Explain the cash flow situation of Googl in Q1 2025.\",\n",
        "        \"What trends can be seen in Googl’s R&D expenses?\",\n",
        "        \"How did inflation affect Googl in 2024?\"\n",
        "      ],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S0jd-7jOJaCa",
        "outputId": "1f23f4ce-bf56-452f-e5c7-76f6bc9a10ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b700a35cda2c47da56.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b700a35cda2c47da56.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v1\n",
            "================================================================================\n",
            "\n",
            "[1/6] Detecting report year...\n",
            "✓ User question year auto-detected: '2024'\n",
            "✓ User question quarterly period auto-detected: 'unknown'\n",
            "\n",
            "[2/6] Building metadata filter...\n",
            "Final filter: {'report_year': {'$eq': '2024'}}\n",
            "\n",
            "[3/6] Performing similarity search...\n",
            "  Query: 'How did inflation affect Googl in 2024?'\n",
            "✓ Found 100 relevant chunks\n",
            "\n",
            "[4/6] Formatting context for LLM...\n",
            "\n",
            "✓ Context formatted for LLM...\n",
            "\n",
            "[5/6] Creating prompt template and composing chain...\n",
            "✓ Prompt template created\n",
            "\n",
            "[5/6] Composing chain...\n",
            "\n",
            "✓ Created prompt and Chain composed!\n",
            "\n",
            "[6/7] Invoking RAG chain with context and query...\n",
            "\n",
            "✓ Answer generated (2583 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v1\n",
            "================================================================================\n",
            "\n",
            "[1/6] Detecting report year...\n",
            "✓ User question year auto-detected: '2024'\n",
            "✓ User question quarterly period auto-detected: '2q'\n",
            "\n",
            "[2/6] Building metadata filter...\n",
            "Final filter: {'$or': [{'report_year': {'$eq': '2024'}}, {'report_quarter': {'$eq': '2q'}}]}\n",
            "\n",
            "[3/6] Performing similarity search...\n",
            "  Query: 'Summarize the Q2 2024 financial performance of Googl.'\n",
            "✓ Found 100 relevant chunks\n",
            "\n",
            "[4/6] Formatting context for LLM...\n",
            "\n",
            "✓ Context formatted for LLM...\n",
            "\n",
            "[5/6] Creating prompt template and composing chain...\n",
            "✓ Prompt template created\n",
            "\n",
            "[5/6] Composing chain...\n",
            "\n",
            "✓ Created prompt and Chain composed!\n",
            "\n",
            "[6/7] Invoking RAG chain with context and query...\n",
            "\n",
            "✓ Answer generated (2908 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v1\n",
            "================================================================================\n",
            "\n",
            "[1/6] Detecting report year...\n",
            "✓ User question year auto-detected: 'unknown'\n",
            "✓ User question quarterly period auto-detected: 'unknown'\n",
            "\n",
            "[2/6] Building metadata filter...\n",
            "Final filter: None\n",
            "\n",
            "[3/6] Performing similarity search...\n",
            "  Query: 'Compare the revenue growth of Googl over the past two years.'\n",
            "✓ Found 100 relevant chunks\n",
            "\n",
            "[4/6] Formatting context for LLM...\n",
            "\n",
            "✓ Context formatted for LLM...\n",
            "\n",
            "[5/6] Creating prompt template and composing chain...\n",
            "✓ Prompt template created\n",
            "\n",
            "[5/6] Composing chain...\n",
            "\n",
            "✓ Created prompt and Chain composed!\n",
            "\n",
            "[6/7] Invoking RAG chain with context and query...\n",
            "\n",
            "✓ Answer generated (2769 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v1\n",
            "================================================================================\n",
            "\n",
            "[1/6] Detecting report year...\n",
            "✓ User question year auto-detected: '2023'\n",
            "✓ User question quarterly period auto-detected: 'annual'\n",
            "\n",
            "[2/6] Building metadata filter...\n",
            "Final filter: {'$or': [{'report_year': {'$eq': '2023'}}, {'report_quarter': {'$eq': 'annual'}}]}\n",
            "\n",
            "[3/6] Performing similarity search...\n",
            "  Query: 'What are the main risks highlighted in Googl’s 2023 annual report?'\n",
            "✓ Found 100 relevant chunks\n",
            "\n",
            "[4/6] Formatting context for LLM...\n",
            "\n",
            "✓ Context formatted for LLM...\n",
            "\n",
            "[5/6] Creating prompt template and composing chain...\n",
            "✓ Prompt template created\n",
            "\n",
            "[5/6] Composing chain...\n",
            "\n",
            "✓ Created prompt and Chain composed!\n",
            "\n",
            "[6/7] Invoking RAG chain with context and query...\n",
            "\n",
            "✓ Answer generated (2471 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v1\n",
            "================================================================================\n",
            "\n",
            "[1/6] Detecting report year...\n",
            "✓ User question year auto-detected: '2025'\n",
            "✓ User question quarterly period auto-detected: '1q'\n",
            "\n",
            "[2/6] Building metadata filter...\n",
            "Final filter: {'$or': [{'report_year': {'$eq': '2025'}}, {'report_quarter': {'$eq': '1q'}}]}\n",
            "\n",
            "[3/6] Performing similarity search...\n",
            "  Query: 'Explain the cash flow situation of Googl in Q1 2025.'\n",
            "✓ Found 100 relevant chunks\n",
            "\n",
            "[4/6] Formatting context for LLM...\n",
            "\n",
            "✓ Context formatted for LLM...\n",
            "\n",
            "[5/6] Creating prompt template and composing chain...\n",
            "✓ Prompt template created\n",
            "\n",
            "[5/6] Composing chain...\n",
            "\n",
            "✓ Created prompt and Chain composed!\n",
            "\n",
            "[6/7] Invoking RAG chain with context and query...\n",
            "\n",
            "✓ Answer generated (2886 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v1\n",
            "================================================================================\n",
            "\n",
            "[1/6] Detecting report year...\n",
            "✓ User question year auto-detected: 'unknown'\n",
            "✓ User question quarterly period auto-detected: 'unknown'\n",
            "\n",
            "[2/6] Building metadata filter...\n",
            "Final filter: None\n",
            "\n",
            "[3/6] Performing similarity search...\n",
            "  Query: 'What trends can be seen in Googl’s R&D expenses?'\n",
            "✓ Found 100 relevant chunks\n",
            "\n",
            "[4/6] Formatting context for LLM...\n",
            "\n",
            "✓ Context formatted for LLM...\n",
            "\n",
            "[5/6] Creating prompt template and composing chain...\n",
            "✓ Prompt template created\n",
            "\n",
            "[5/6] Composing chain...\n",
            "\n",
            "✓ Created prompt and Chain composed!\n",
            "\n",
            "[6/7] Invoking RAG chain with context and query...\n",
            "\n",
            "✓ Answer generated (2652 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b700a35cda2c47da56.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}